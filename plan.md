## **Стек**

* Frontend: простой React/Vue или даже чистый HTML

* Backend: FastAPI

* Worker: Celery/RQ \+ Redis

* XLSX: openpyxl

* Индекс категорий: rapidfuzz (старт) → FAISS/pgvector (если объёмы большие)

* LLM: Deepseek или GPT-4.1 (или split на mini)

  ## 

  ## **План технической реализации** 

  ### **1\) Юзер открывает сайт сервиса**

**Frontend (Web UI)**

* Страница: загрузка файла \+ кнопка “Обработать”.

* Показ статуса: “Файл загружен → Разделяю комментарии → Проставляю категории → Готово”.

* После готовности: кнопка “Скачать результат”.

**Backend**

* Отдаёт статичный фронт и API.

  ---

  ### **2\) Юзер загружает файл**

**API**

* `POST /jobs` (multipart/form-data: файл `.xlsx`)

  * Ответ: `{ job_id }`

**Backend действия**

* Проверка формата: `.xlsx`, размер, защита от вирусов (по желанию).

* Сохранение файла во временное хранилище (например, `uploads/{job_id}.xlsx` или S3).

* Создание задачи в очереди: `ProcessJob(job_id, file_path)`.

  ---

  ### **3\) Система считывает файл и начинает обработку и разделение комментариев**

**Worker: этап Split (обязательно “каждая ячейка комментария”)**

#### **3.1 Чтение XLSX**

* Открываем входной XLSX.

* Находим лист с данными (фиксированный или первый).

* Находим столбец `КОММЕНТАРИЙ` (по точному названию / маппингу).

* Считываем все строки в структуру (DataFrame/список dict).

  #### **3.2 Разделение комментариев (Split)**

Для **каждой строки**:

* берём `comment = row["КОММЕНТАРИЙ"]`

* если пусто/“нет замечаний” → `defects=[]`

* иначе вызываем LLM Split **с жёстким JSON-ответом**:

  * результат: `{"defects":[{"text":"..."}, ...]}`

**Оптимизация без потери “каждая ячейка обработана”:**

* Батчим комментарии пачками (например 20–50 штук за 1 запрос), но **каждый комментарий всё равно отдельно разобран**.

* Кэшируем split по `hash(comment)` (часто комментарии повторяются).

  #### **3.3 Expand (размножение строк)**

* На каждый `defect.text` создаём отдельную строку результата:

  * все столбцы копируем 1:1

  * `КОММЕНТАРИЙ = defect.text`

* Если дефектов 5 → получаем 5 строк с одинаковыми полями, кроме `КОММЕНТАРИЙ`.

На этом шаге сохраняем промежуточный результат (в памяти или временно на диск), чтобы не потерять прогресс.

---

### **4\) После завершения, система начинает анализировать файл с категориями (общий, хранится на сервере)**

**Справочник категорий**

* Лежит на сервере, например: `data/categories.xlsx` (или CSV/БД).

* Общий для всех файлов.

  #### **4.1 Подготовка справочника (однократно при старте сервиса)**

Чтобы не “анализировать” его заново для каждого job:

* При запуске backend/worker:

  * читаем файл категорий

  * нормализуем список (trim, уникальность)

  * строим индекс для поиска top-N кандидатов:

    * минимум: fuzzy matching

    * лучше: векторный индекс (эмбеддинги)

* Держим в памяти \+ следим за изменениями файла (по времени модификации/хэшу):

  * если файл обновили → пересобрать индекс

Для пользователя это выглядит как “после split система берёт общий файл категорий”, а технически вы экономите время.

---

### **5\) Проставляет категории к каждому комментарию**

**Worker: этап Classify**

Для каждой строки результата (после expand), где `КОММЕНТАРИЙ` уже атомарный:

1. Retrieve: ищем **top-N** наиболее подходящих категорий из общего справочника (например N=10).

2. LLM Classify:

   * на вход: `defect_text` \+ список кандидатов (строго)

   * модель возвращает JSON: `{"chosen":"точное_название_категории_из_списка"}`

3. Пишем в новый столбец: **`Категория дефекта`**

**Оптимизации:**

* Батчим классификацию пачками (например 50 дефектов за запрос).

* Кэшируем classify по `hash(defect_text)` (и по версии справочника).

**Контроль качества (очень желательно):**

* Добавить тех.колонку `confidence` (0–1) и `needs_review` если ниже порога.

* Либо хотя бы логировать top-3 кандидата для спорных случаев.

  ---

  ## **Выходной файл и отдача пользователю**

  ### **Формирование результата**

* Создаём выходной XLSX:

  * Лист `Result` или `Разделено`

  * Все исходные столбцы \+ новый `Категория дефекта`

* Сохраняем: `results/{job_id}_processed.xlsx`

  ### **API выдачи**

* `GET /jobs/{job_id}` → `{ status, progress, download_url }`

* `GET /jobs/{job_id}/download` → скачивание файла


